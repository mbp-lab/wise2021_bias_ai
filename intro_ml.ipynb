{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Building a Machine Learning Model  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will show you the basics of training a machine learning algorithm.  For this example, we will perform __supervised learning__ by training __Support Vector Machines__, a common ML algorithm, __detect music genres from audio files__. \n",
    "\n",
    "To train the models we will use two datasets consisting of audio features for the task of **genre classification**. The audio features are calculated from audio (.WAV) files using [Marsyas](http://marsyas.info/index.html#), an open source framework for audio analysis. \n",
    "\n",
    "The first dataset contains just two song level features (average spectral centroid and average spectral rolloff). Using only two features we can directly visualize the data with a scatter plot. The points are colored in terms of their class membership.  \n",
    "\n",
    "The second dataset contains the same songs but with a set of 124 spectral features.\n",
    "\n",
    "There are three genres each represented by a 100 audio tracks or points in this case. The genres are __classical, jazz and metal__. \n",
    "\n",
    "Through the process we will get an intro to the ML pipeline, common ML tools (numpy, pandas, scikit-learn, matplotlib), and running a Jupyter Notebook (which will be needed later in the semester).\n",
    "\n",
    "We will use [scikit-learn](https://scikit-learn.org/stable/index.html), a common ML framework for Python, to implement the ML pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Standard Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Numpy for vector processing and linear algebra\n",
    "import numpy as np\n",
    "\n",
    "# Scikit learn for ML Pipeline\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "# Matplotlib + Seaborn for plotting\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Panda for data processing\n",
    "import pandas as pd\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "\n",
    "# Other Utils\n",
    "from utils import make_mesh\n",
    "from data.data_utils import get_doughnut_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Generation\n",
    "\n",
    "<img src=\"images/ml_pipeline_data.png\" width=1200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Collection and Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### _Methods for Collection and Annotation_\n",
    "- __Manual Annotation__\n",
    "- __Empirical Studies / Experiments__\n",
    "  - physiological measurements from the Trier Social Stress Test\n",
    "- __Web Scraping__\n",
    "  - search Google Images for pictures of specified facial expressions, such as _happy_ or _sad_\n",
    "- __Historical Data and Records__\n",
    "  - Risk assessment systems\n",
    "- __Crowdsourcing__\n",
    "  - [Amazon Mechanical Turk](https://onlinelibrary.wiley.com/doi/abs/10.1002/bdm.1753?casa_token=iW56bog9MJAAAAAA:BnSAAMDrpDFfnwMvphaHyfatw4W1f5q1RPT3KuhZitEYpNX1fDoBC7nRvNUvANvF5nFQvsO_d8WchavK)\n",
    "  - [Data annotation games](https://www.cs.cmu.edu/~elaw/papers/ISMIR2007.pdf)\n",
    "- __Existing Datasets__\n",
    "  - [ImageNet](https://www.image-net.org/)\n",
    "  \n",
    " __What are the potential issues with these methods for data collection?__\n",
    " \n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### [GTZAN Music Genre Dataset](https://www.tensorflow.org/datasets/catalog/gtzan)\n",
    "\n",
    "For this example we will use an existing dataset.\n",
    "\n",
    "Tzanetakis et al. (2001) \"Automatic Musical Genre Classification Of Audio Signals\"\n",
    "\n",
    "- Music selected from author's personal music library\n",
    "- 1000 audio tracks, 30 seconds each\n",
    "- 10 Genres, each with 100 tracks\n",
    "  - blues, classical, country, disco, hiphop, jazz, metal, pop, reggae, rock\n",
    "  \n",
    "(We're using a subset of 300 tracks from 3 genres: classical, metal and rock)\n",
    "  \n",
    "##### Feature Extraction\n",
    "\n",
    "- Spectral analysis of raw audio file using Marsyas\n",
    "- Our example dataset contains 124 spectral features describing audio content\n",
    "  - including: time-domain Zero-Crossings, Spectral Centroid, Rolloff, Flux and Mel-Frequency Cepstral Coefficients (MFCC)\n",
    "  \n",
    "**_What are some potential causes of biases in this dataset?_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Load Reduced Feature and Full Feature Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load dataset with only two features for visualization\n",
    "X_sm, y_sm = datasets.load_svmlight_file(\"data/3genres.arff.libsvm\")\n",
    "X_sm = X_sm.toarray()\n",
    "print(f'X: Samples={X_sm.shape[0]}, Features={X_sm.shape[1]}')\n",
    "print(f'y: Samples={y_sm.shape[0]}')\n",
    "\n",
    "# save features and labels to Pandas dataframe for easier processing\n",
    "target_names = ['classical', 'jazz', 'metal']\n",
    "features_sm = ['centroid', 'rolloff']\n",
    "label = 'genre'\n",
    "df_sm = pd.DataFrame(data=X_sm, columns=features_sm)\n",
    "df_sm[label] = y_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load dataset with only with all features\n",
    "X_full, y_full = datasets.load_svmlight_file(\"data/3genres_full.arff.libsvm\")\n",
    "X_full = X_full.toarray()\n",
    "print(f'X: Samples={X_full.shape[0]}, Features={X_full.shape[1]}')\n",
    "print(f'y: Samples={y_full.shape[0]}')\n",
    "\n",
    "# save features and labels to Pandas dataframe for easier processing\n",
    "target_names = ['classical', 'jazz', 'metal']\n",
    "features_full = list(range(X_full.shape[1]))\n",
    "label = 'genre'\n",
    "df_full = pd.DataFrame(data=X_full, columns=features_full)\n",
    "df_full[label] = y_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Review Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sm[features_sm].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full[features_full].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Preperation / Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Data Processing__\n",
    "\n",
    "- standardizing and normalizing data\n",
    "- feature extraction (grey line between data collection and processing)\n",
    "- data augmentation\n",
    "\n",
    "##### __Data Preperation__\n",
    "- Reviewing and cleaning data samples\n",
    "- Spliting data into Training, Testing and Validation (common for deep learning) datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Min Max Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale each feature, $i$, to range $[min, max]$, using following formula:\n",
    "\n",
    "scale between $[0, 1]$\n",
    "$$\n",
    "    x_i^{\\prime} = \\dfrac{x_i - min(x_i)}{max(x_i) - min(x_i)}\n",
    "$$\n",
    "\n",
    "then scale to arbitrary min-max values in range $[min, max]$\n",
    "$$\n",
    "    x_i^{\\prime} = x_i^{\\prime} * (max - min) + min\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_scaler = preprocessing.MinMaxScaler()  # default range [0, 1]\n",
    "X_minmax = minmax_scaler.fit_transform(df_sm[features_sm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_minmax = pd.DataFrame(data=X_minmax, columns=features_sm)\n",
    "X_minmax.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Standardize Data or Zero Mean Unit Variance Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removes the mean of the data and scales to have unit variance\n",
    "\n",
    "$$\n",
    "    x_i^{\\prime} = \\dfrac{x_i - \\mu_i}{\\sigma_i}\n",
    "$$\n",
    "\n",
    "where $\\mu$ is the mean and $\\sigma$ is the standard deviation of feature $i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler = preprocessing.StandardScaler()\n",
    "X_zmuv = standard_scaler.fit_transform(df_sm[features_sm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_zmuv = pd.DataFrame(data=X_zmuv, columns=features_sm)\n",
    "X_zmuv.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "####  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Train/Test Split\n",
    "\n",
    "- Split dataset into train and testing sets\n",
    "- apply normalization preprocessing to datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features and labels from original df\n",
    "# small dataset\n",
    "X = np.array(df_sm[features_sm])\n",
    "y = np.array(df_sm[label])\n",
    "\n",
    "# full dataset\n",
    "# X = np.array(df_full[features_full])\n",
    "# y = np.array(df_full[label])\n",
    "\n",
    "# Split the data for training and testing\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=.25, random_state=5)\n",
    "\n",
    "# initialize and fit scaler to training data\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# scale training and testing data using scaler fit on training data\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print('Training Data')\n",
    "print(f'X: {X_train.shape}, y: {y_train.shape}')\n",
    "print('Test Data')\n",
    "print(f'X: {X_test.shape}, y: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data Statistics\n",
    "df_tmp = pd.DataFrame(X_train)\n",
    "df_tmp.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data Statistics\n",
    "df_tmp = pd.DataFrame(X_test)\n",
    "df_tmp.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Questions\n",
    "1. Why is it important to scale/normalize the test data scaled separately with training data statistics?\n",
    "2. What is the problem with splitting the data randomly? \n",
    "    - How can this be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Training  \n",
    "\n",
    "<img src=\"images/ml_pipeline_models.png\" width=1200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 1. Model Type and Architecture\n",
    "\n",
    "- Depends on Task, _T_\n",
    "    - Classification, Regression, Clustering, etc...\n",
    "- Model Type\n",
    "    - Classical ML\n",
    "        - Support Vector Machines, Linear or Logistic Regression, Decision Trees, etc...\n",
    "    - Deep Learning\n",
    "        - Feedfordward network, Convolutional Network, ResNet, Transformers, etc...\n",
    "\n",
    "##### 2. Hyperparameter selection\n",
    "\n",
    "- A parameter of selected model and architecture that affects the learning process\n",
    "- Used to tuning the model training process\n",
    "\n",
    "##### 3. Perform Training\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Create and Train SVMs with different Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A support vector machine (SVM) is a linear model that supports non-linear datasets through the [kernel trick](http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html) (also see optional section, _More on SVMs Maximum Margin Seperating Hyperplane_, below).\n",
    "\n",
    "Scikit-Learn Support Vector Machine Implementations\n",
    "\n",
    "- [Support Vector Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
    "- [Linear Suppor Vector Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.htmlhttps://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Model and Hyperparameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = [\"linear\", \"rbf\", \"poly\"]\n",
    "C = .3 # regularization parameter\n",
    "poly_degree = 3 # polynomial order for the poly kernel\n",
    "\n",
    "clfs = {\n",
    "    \"SVC w/ Linear Kernel\": svm.SVC(kernel=\"linear\", C=C),\n",
    "    \"LinearSVC\": svm.LinearSVC(C=C),\n",
    "    \"SVC w/ RBF Kernel\": svm.SVC(kernel=\"rbf\", C=C),\n",
    "    \"SVC w/ 3 Degree Polynomial kernel\": svm.SVC(kernel=\"poly\", degree=poly_degree, C=C)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the models\n",
    "models = {title: clf.fit(X_train, y_train) \n",
    "          for title, clf in clfs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Plot the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, sub = plt.subplots(2,2, figsize=(20, 14))\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.2)\n",
    "\n",
    "# make mesh for plotting\n",
    "X_normed = scaler.transform(X) # scale X to ensure mesh has correct bounds\n",
    "xx, yy = make_mesh(X_normed[:, 0], X_normed[:, 1], padding=.1, h=.005)\n",
    "\n",
    "for (title, clf), ax in zip(models.items(), sub.flatten()):\n",
    "    \n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    test_score = clf.score(X_test, y_test)\n",
    "    train_score = clf.score(X_train, y_train)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    ax.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap=plt.cm.coolwarm, zorder=11, edgecolor='k', s=20)\n",
    "    # ax.scatter(X_test[:,0], X_test[:,1], c=y_test, s=50, zorder=10, edgecolor='k',cmap=plt.cm.coolwarm)\n",
    "\n",
    "    if title != \"LinearSVC\":\n",
    "        sv = clf.support_\n",
    "        X_support = X_train[sv]\n",
    "        y_support = y_train[sv]\n",
    "        ax.scatter(X_support[:,0], X_support[:, 1], c=y_support, cmap=plt.cm.coolwarm, \n",
    "                   s=100, zorder=10, edgecolor='k')\n",
    "\n",
    "    ax.contourf(xx, yy, Z, cmap=plt.cm.coolwarm)\n",
    "    ax.set_title(f'{title}: Training Acc: {train_score*100:.2f}, Test Acc: {test_score*100:.2f}')\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluation models for with the goal of ___generalizability___\n",
    "- Metric selection typically depends on task\n",
    "\n",
    "##### __Some common metrics are:__\n",
    "- __Accuracy:__ percent of correctly labeled predictions\n",
    "  - correct / total\n",
    "  - error rate is the inverse (1 - acc)\n",
    "- __Precision:__ proportion of positively labeled predictions that are correct\n",
    "  - tp / (tp + fp)  \n",
    "- __Recall:__ proportion of actual positives that are identified correctly\n",
    "  - tp / (tp + fn)\n",
    "- __F1 Score:__ harmonic average of precision and recall\n",
    "  - 2 * (precision * recall) / (precision + recall)\n",
    "  \n",
    "  \n",
    "##### __Confusion Matrics__\n",
    "\n",
    "- Matrix of actual vs. predicted labels for each class\n",
    "- Provides further intuition as to where model is confused\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Evaluate Trained SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for title, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(title)\n",
    "    print(metrics.classification_report(y_test, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Plot Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 14))\n",
    "for i, (title, model) in enumerate(models.items()):\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    \n",
    "    df_cm = pd.DataFrame(cm, range(3), range(3))\n",
    "    sns.set(font_scale=1.4) # for label size\n",
    "    sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, cmap='Blues',\n",
    "                xticklabels=target_names, yticklabels=target_names) # font size\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Cross Validation with Hyperparameter Selection](https://towardsdatascience.com/cross-validation-and-hyperparameter-tuning-how-to-optimise-your-machine-learning-model-13f005af9d7d)\n",
    "\n",
    "A better method for selecting and evaluating models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### More on SVMs Maximum Margin Seperating Hyperplane (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_blobs, y_blobs = make_blobs(n_samples=40, centers=2, random_state=6)\n",
    "X_doughnut, y_doughnut, _, _ = get_doughnut_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X, y = X_doughnut, y_doughnut\n",
    "X, y = X_blobs, y_blobs\n",
    "\n",
    "# fit the model, don't regularize \n",
    "clf = svm.SVC(kernel='linear', C=1) ## Try with different Kernels\n",
    "clf.fit(X, y)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired, zorder=10)\n",
    "\n",
    "# plot the decision function\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "# create grid to evaluate model\n",
    "xx, yy = make_mesh(X[:, 0], X[:, 1], padding=0, h=.05)\n",
    "Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# plot decision boundary and margins\n",
    "ax.contour(xx, yy, Z, levels=[-1, 0, 1], alpha=0.5, colors='k',\n",
    "           linestyles=['--', '-', '--'], zorder=11)\n",
    "# # plot support vectors\n",
    "sc = ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,\n",
    "           linewidth=1, facecolors='none', edgecolor='k', zorder=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model Postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Postprocessing__\n",
    "\n",
    "- Selection of probablity (or score) threshold for final class determination\n",
    "  - often a tradeoff between precision and recall\n",
    "- Ensemble Aggregration\n",
    "- Sample Aggregation\n",
    "  - Majority voting vs. Max Value vs. Average\n",
    "  \n",
    "_Genre Classification Example of Sample Aggregation_\n",
    "\n",
    "- Instead of one feature vector per 30 second clip, we extract features every 5 seconds for training\n",
    "- How do we aggregate the results of all 6 clips into a final score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Challenges in Real World Deployment__\n",
    "\n",
    "- Real world setting vs. data collection setting\n",
    "    - Domain Shift / Data Drift\n",
    "- Deployment Requirements\n",
    "    - Explainabilty\n",
    "    - Latency vs. Real-time feedback\n",
    "- Unintended or undesired uses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
